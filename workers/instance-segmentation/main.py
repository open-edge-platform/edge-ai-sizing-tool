# Copyright (C) 2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

import os
import re
import sys 
import cv2 
import time 
import math 
import socket 
import signal 
import logging 
import uvicorn 
import zipfile 
import requests 
import argparse 
import threading 
import urllib.parse
import urllib.request
import numpy as np 
import subprocess as sp 
from segmentation_models_download import SEGMENTATION_MODELS, export_model
from pathlib import Path 
from fastapi import FastAPI 
from contextlib import asynccontextmanager
# from yolo_download import export_yolo_model
from fastapi.middleware.cors import CORSMiddleware 
from fastapi.responses import StreamingResponse, JSONResponse

#configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

#set up ennvironment variables to enable dlstreamer
os.environ["LIBVA_DRIVER_NAME"] = "iHD"
os.environ["GST_PLUGIN_PATH"] = (
    "/opt/intel/dlstreamer/lib:/opt/intel/dlstreamer/gstreamer/lib/gstreamer-1.0:/opt/intel/dlstreamer/streamer/lib/"
)
os.environ["LD_LIBRARY_PATH"] = (
"/opt/intel/dlstreamer/gstreamer/lib:/opt/intel/dlstreamer/lib:/opt/intel/dlstreamer/lib/gstreamer-1.0:/sr/lib:/opt/intel/dlstreamer/lib:/usr/local/lib/gstreamer-1.0:/usr/local/lib")
os.environ["LIBVA_DRIVERS_PATH"] = "/usr/lib/x86_64-linux-gnu/dri"
os.environ["GST_VA_ALL_DRIVERS"] = "1"
os.environ["PATH"] = (
   f"/opt/intel/dlstreamer/gstreamer/bin:/opt/intel/dlstreamer/bin:{os.environ['PATH']}"
)

env = os.environ.copy()
venv_path = os.path.dirname(sys.executable)
venv_bin = str(Path(sys.executable).parent)
env["PATH"] = f"{venv_path}:{env['PATH']}"

VIDEO_DIR = Path('../assets/media')
MODEL_DIR = Path("./models")
CUSTOM_MODELS_DIR = Path("../custom_models/instance-segmentation-(DLStreamer)")
RSTP_SERVER_URL = "rtsp://localhost:8554"

pipeline_process = None 
ffmpeg_process = None

app = FastAPI()

def is_valid_id(workload_id):
    """
    Validate the workload ID to prevent URL manipulation and ensure it is a positive integer
    """
    if isinstance(workload_id, int) and workload_id >= 0:
        return True 
    return False

def update_payload_status(workload_id: int, status):
    """Update the workload status in a safe way, allow-listing scheme, authority,
    and preventing unsafe path traversal."""
    if not is_valid_id(workload_id):
        logging.error(f"Invalid workload ID: {workload_id}. Refusing to update status.")
        return 
    
    #Hardcode scheme & authority (safe allow-list)
    allowed_scheme = "http"
    allowed_netloc = "127.0.0.1:8080"
    
    #Build the path carefully. Rejet characters such as "../"
    path = f"/api/workloads/{workload_id}"
    
    #Use urllib.parse to verify
    composed_url = f"{allowed_scheme}://{allowed_netloc}{path}"
    parsed_url = urllib.parse.urlparse(composed_url)

    #enforce scheme & authoriy are what we expect
    if parsed_url.scheme != allowed_scheme or parsed_url.netloc != allowed_netloc:
        logging.error(f"URL scheme or authority not allowed: {parsed_url.geturl()}")
    
    #basic check for path traversl attempts (../, //, whitespace, etc.)
    if ".." in path or "//" in path or " " in path:
        logging.error(f"Invalid characters in URL path: {path}")
        return 
    
    #now safe to use
    url = parsed_url.geturl()
    data = {"status": status, "port" : args.port}
    try:
        response = requests.patch(url, json=data)
        response.raise_for_status()
        logging.info(f"Successfully updated to {status} for {workload_id}.")
    except requests.exceptions.RequestException as e:
        logging.info(f"Failed to update status: {e}")

@asynccontextmanager
async def lifespan(app: FastAPI):
    logging.info("--- Initializing instance segmentation worker ---")
    app.state.pipeline_metrics = {
        "total_fps" : None,
        "number_streams": None,
        "average_fps_per_streams" : None,
        "fps_streams" : None,
        "timestamp": None
    }
    thread = threading.Thread(target=main, daemon=True)
    thread.start()
    yield 
    logging.info("--- Shutting down instance segmentation worker ---")

app = FastAPI(lifespan=lifespan)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://127.0.0.1:8080", "http://localhost:8080"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)



def parse_arguments():
    parser = argparse.ArgumentParser(description="FastAPI server for IntelÂ® DLStreamer instance segmentation model")
    parser.add_argument(
        "--input",
        type=str,
        default=f"{VIDEO_DIR}/people-detection.mp4",
        help="Input source e.g. /dev/video0, videofile.mp4, etc"
    )
    parser.add_argument(
        "--inference_mode",
        type=str,
        default="gvadetect",
        help="Inference mode: gvadetect or gvaclassify (default: gvadetect)",
    )
    parser.add_argument(
        "--model",
        type=str,
        default="mask_rcnn_inception_resnet_v2_atrous_coco",
        help="Model name (default: mask_rcnn_inception_resnet_v2_atrous_coco)"
    )
    parser.add_argument(
        "--model_parent_dir",
        type=str,
        default=MODEL_DIR,
        help=f"Path to the model directory (Default: {MODEL_DIR})",
    )
    parser.add_argument(
        "--model_precision",
        type=str,
        default="FP16",
        help="Model precision (default: FP16)",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="CPU",
        help="Device to run inference on (default:CPU)",
    )
    parser.add_argument(
        "--decode_device",
        type=str,
        default="CPU",
        help="Device to run decode on (default: CPU)",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=1,
        help="Batch size for inference (default: 1)",
    )
    parser.add_argument(
        "--tcp_port",
        type=int, 
        default=5001,
        help="Port to spawn the DLStreamer pipeline (default: 5001)",
    )
    parser.add_argument(
        "--port",
        type=int,
        default=5998,
        help="Port to run the FastAPI server on (default: 5998)",   
    )
    parser.add_argument(
        "--id",
        type=int,
        help="Workload ID to update the workload status"
    )
    parser.add_argument(
        "--number_of_streams",
        type=int,
        default=1,
        help="Number of streams to run (default: 1)"
    )
    parser.add_argument(
        "--width_limit",
        type=int,
        default=640,
        help="Width limit for the video stream (default: 640)"
    )
    parser.add_argument(
        "--height_limit",
        type=int,
        default=480,
        help="Height is limit for the video stream (default: 480)",
    )
    parser.add_argument(
        "--rows",
        type=int,
        default=None,
        help="Number of rows for the compositor grid (default: None)",
    )
    parser.add_argument(
        "--cols",
        type=int,
        default=None,
        help="Number of columns for the compositor grid (default: None)",
    )
    return parser.parse_args()

args = parse_arguments()


def build_compositor_props(num_streams, final_width, final_height, rows=None, cols=None):
    """
    Method to dynamically split a single final_width * final_height compositor output window into a grid of N sub-windows.
    """
    if not rows or not cols:
        # Determine how many columns and rows a square/grid grid would need
        cols = math.ceil(math.sqrt(num_streams))
        rows = math.ceil(num_streams / cols)

    # Calculate width and height for each sub-window
    sub_width = final_width // cols
    sub_height = final_height // rows

    comp_props = []
    for i in range(num_streams):
        row = i // cols
        col = i % cols
        
        x_pos = col * sub_width
        y_pos = row * sub_height

        comp_props.append(
            f"sink_{i}::xpos={x_pos} sink_{i}::ypos={y_pos} sink_{i}::width={sub_width} sink_{i}::height={sub_height}"
        )

    return " ".join(comp_props)

def build_pipeline(model_full_path, inference_mode, input, device, decode_device, tcp_port, number_of_streams=1, batch_size=1, model_proc_path=None, model_label_path=None, rows=None, cols=None):
        """Build the DLStreamer pipeline for MJPEG streaming"""
        # Determine source element based on input
        if input.endswith((".mp4", ".avi", ".mov")):
            source_command = ["filesrc", f"location={input}", "loop=true"]
        elif input.startswith("rtsp://"):
            source_command = ["rtspsrc", f"location={input}",  "protocols=tcp"]
        elif input.startswith("/dev/video"):
            if number_of_streams > 1:
                source_command = [
                    "v4l2src",
                    f"device={input}",
                    "!",
                    "videoconvert",
                    "!",
                    "tee",
                    "name=camtee",
                    "!",
                    "multiqueue",
                    "name=camq",
                ]
            else:
                source_command = ["v4l2src", f"device={input}"]
        else:
            logging.error(f"Unsupported input source: {input}")
            return None
        
        #Configure decode element
        if "CPU" in decode_device:
            if input.startswith("/dev/video"):
                decode_element = ["videoconvert",]
                caps_element = ["video/x-raw"]
            else:
                decode_element = ["rtph264depay", "!", "avdec_h264", "!", "videoconvert",]
                caps_element = ["video/x-raw,format=BGR"]
        elif "GPU" in decode_device or "NPU" in decode_device :
            if input.startswith("/dev/video"):
                decode_element = ["videoconvert", "!", "vapostproc",]
                caps_element = ["video/x-raw(memory:VAMemory)"]
            else:
                decode_element = ["rtph264depay", "!", "avdec_h264", "!", "vapostproc",]
                caps_element = ["video/x-raw(memory:VAMemory),format=NV12"]
        else:
            logging.error(f"Unsupported device: {decode_device}")
            return None 
        
        inference_command = [
            f"{inference_mode}",
            f"model={model_full_path}",
            f"device={device}",
        ]
        
        #if dont have model proc file then we make it use it without
        # Add model proc file if available
        if model_proc_path is not None and os.path.exists(model_proc_path):
            logging.info(f"Using model proc file: {model_proc_path}")
            inference_command.append(f"model-proc={model_proc_path}")
        else:
           logging.warning("No model proc file found. Proceeding without one.")

        if model_label_path is not None and os.path.exists(model_label_path):
            logging.info(f"Using model label file: {model_label_path}")
            inference_command.append(f"labels-file={model_label_path}")
        else:
           logging.warning("No model label file found. Proceeding without one.")

        #pre-processing
        if ("GPU" in decode_device and "GPU" in device) or ("NPU" in decode_device and "NPU" in device):
            inference_command.append(f"batch-size={batch_size}")
            inference_command.append("nireq=4")
            inference_command.append("pre-process-backend=va-surface-sharing")
        elif "GPU" in decode_device and "CPU" in device:
            inference_command.append("pre-process-backend=va")
        else: 
            inference_command.append("pre-process-backend=ie")


        #beginning of piepline
        pipeline = ["gst-launch-1.0"]
        
        comp_props_str = build_compositor_props(
            args.number_of_streams, args.width_limit, args.height_limit, args.rows, args.cols
        )
        comp_props = comp_props_str.split()
        logging.info(f"Compositor properties: {comp_props}")
        
        pipeline += ["compositor", "name=comp"] + comp_props
        pipeline += ["!", "queue"]
        pipeline += ["!", "jpegenc"]
        pipeline += ["!", "multipartmux", "boundary=frame"] 
        pipeline += ["!", "tcpserversink", f"host=127.0.0.1", f"port={tcp_port}",]
        
        if input.startswith("/dev/video") and number_of_streams > 1:
            #for multiple webcam streams, use tee to split the source
            pipeline.append("sync=false")
            pipeline += source_command
            for i in range(number_of_streams):
                pipeline += [f"camtee.", "!", "queue", "max-size-buffers=10", "leaky=downstream"]
                pipeline += ["!",  *decode_element]
                pipeline += ["!", *caps_element]
                pipeline += ["!", *inference_command]
                pipeline += ["!", "queue", "!", "gvafpscounter", "!" "videoconvert",  "!", "gvawatermark", "!", "videoconvert", "!", "video/x-raw", "!", f"comp.sink_{i}"]   
        else:
           for i in range(number_of_streams):
                pipeline += source_command
                pipeline += ["!", *decode_element]
                pipeline += ["!", *caps_element]
                pipeline += ["!", *inference_command]
                pipeline += ["!", "queue", "!", "gvafpscounter", "!", "videoconvert", "!", "gvawatermark", "!", "videoconvert", "!", "video/x-raw", "!", f"comp.sink_{i}"]     

        #log the pipepline
        logging.info(f"Full pipeling: {' '.join(pipeline)}\n")
        return pipeline
    

def run_pipeline(pipeline):
    """
    Run the GStreamer pipeline and process its output in real-time.
    Handles EOS for looping and updates pipeline metrics.
    """
    logging.info("Starting GStreamer pipeline...")
    try:
        process = sp.Popen(pipeline, stdout=sp.PIPE, stderr=sp.PIPE, text=True)
        # Monitor the pipeline's stdout
        for line in process.stdout:
            logging.info(line.strip())
            # Process each line of output using filter_result
            metrics = filter_result_fps(line.strip())
            if metrics:
                app.state.pipeline_metrics.update(metrics)

        # Capture any errors from stderr
        for error_line in process.stderr:
            logging.error(error_line.strip())

        # Check if the process exited due to EOS
        if process.returncode == 0 or process.returncode is None:
            logging.info("Pipeline reached EOS. Restarting...")
            process.communicate()
        else:
            logging.error(f"Pipeline exited with error code: {process.returncode}")

    except Exception as e:
        logging.error(f"Unexpected error: {e}")
        if process and process.poll() is None:
            process.terminate()
            process.wait()
    finally:
        if process and process.poll() is None:
            process.terminate()
            process.wait()


def stop_signal_handler(sig, frame):
    """
    Signal handler for SIGINT to terminate worker gracefully.
    """
    logging.info("SIGINT received. Stopping the application...")
    global pipeline_process, ffmpeg_process
    if pipeline_process and pipeline_process.poll():
        pipeline_process.terminate()
        pipeline_process.wait()
    if ffmpeg_process and ffmpeg_process.poll() is None:
        ffmpeg_process.terminate()
        ffmpeg_process.wait()
    
    sys.exit(0)
signal.signal(signal.SIGINT, stop_signal_handler)


def filter_result_fps(output):
    """
    Extract the FPS metrics from the command output
    """
    fps_pattern = re.compile(
        r"FpsCounter\(.*\): total=(\d+\.\d+) fps, number-streams=(\d+), per-stream=(\d+\.\d+) fps(?: \((.*?)\))?"
    )
    match = fps_pattern.search(output)
    if match:
        total_fps = float(match.group(1))
        number_streams = int(match.group(2))
        average_fps_per_stream = float(match.group(3))
        all_streams_fps_str = match.group(4)

        if all_streams_fps_str:
            all_streams_fps = [float(x.strip()) for x in all_streams_fps_str.split(",")]
        else:
            all_streams_fps = [average_fps_per_stream]

        fps_streams = {
            f"stream_id {i+1}": fps_val
            for i, fps_val in enumerate(all_streams_fps[:number_streams])
        }

        return {
            "total_fps": total_fps,
            "number_streams": number_streams,
            "average_fps_per_stream": average_fps_per_stream,
            "fps_streams": fps_streams,
            "timestamp": time.time(),
        }
    return None

def is_rtsp_stream_running(rtsp_url, retries=5, delay=1):
    """
    Check if an RTSP stream is running by attempting to connect to it.
    """
    for attempt in range(retries):
        try:
            cap = cv2.VideoCapture(rtsp_url)
            if cap.isOpened():
                ret, _ = cap.read()
                cap.release()
                return True
            else:
                logging.warning(f"RTSP stream not ready at {rtsp_url}, attempt {attempt+1}/{retries}")
        except Exception as e:
            logging.error(f"Error checking RTSP stream: {e}. Retrying ... ({attempt + 1}/{retries})")
        time.sleep(delay)
    return False
                
def is_valid_video_file(filepath):
    """
    Check if a file exists and is a valid video file.
    """
    if not os.path.exists(filepath):
        return False 
    
    try:
        cap = cv2.VideoCapture(filepath)
        opened = cap.isOpened()
        cap.release()
        return opened
    except:
        return False 
    

def mjpeg_stream(host: str ="127.0.0.1", port: int = 5001, retries: int = 5, delay: int = 1):
    """
    Connect to the GStreamer TCP server and yield MJPEG frames.
    """
    for attempt in range(retries):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as client_socket:
                client_socket.settimeout(5)
                client_socket.connect((host, port))
                logging.info(f"Connected to MJPEG stream on port {port}")
                buffer = b""

                while True:
                    # Read data from the TCP server
                    data = client_socket.recv(4096)
                    if not data:
                        break

                    buffer += data
                    
                    while b"\r\n\r\n" in buffer:
                        frame, _, buffer = buffer.partition(b"\r\n\r\n")

                        try:
                            # Convert raw frame data to a NumPy array and decode the frame
                            np_frame = np.frombuffer(frame, dtype=np.uint8)
                            image = cv2.imdecode(np_frame, cv2.IMREAD_COLOR)
                            if image is None:
                                continue

                            _, jpeg_frame = cv2.imencode(".jpg", image)

                            # Yield the encoded JPEG frame
                            yield (
                                b"--frame\r\n"
                                b"Content-Type: image/jpeg\r\n\r\n"
                                + jpeg_frame.tobytes()
                                + b"\r\n"
                            )
                        except Exception as e:
                            logging.error(f"Error processing frame: {e}")
                            
        except ConnectionRefusedError:
            logging.warning(f"Connection refused. Retrying in {delay}s... ({attempt + 1}/{retries})")
            time.sleep(delay)
        except (socket.timeout, BrokenPipeError, ConnectionResetError) as e:
            logging.error(f"Socket error: {e}. Retrying in {delay}s... ({attempt + 1}/{retries})")
            time.sleep(delay)
        except Exception as e:
            logging.error(f"An unexpected error occurred in MJPEG stream: {e}")
            break
    logging.error(f"Failed to connect to MJPEG stream on port {port} after {retries} attempts.")
    update_payload_status(args.id, status="failed")
    yield b"--frame\r\nContent-Type: text/plain\r\n\r\nStream not available. Check worker logs.\r\n"

def main():
    """
    Main function to start the GStreamer pipeline.
    """

    model_proc_path = None
    model_label_path = None
    
    logging.info(f"View stream at url: http://localhost:{args.port}/result/{args.tcp_port}")

    if os.path.realpath(args.input) != os.path.abspath(args.input):
        logging.info(f"Error: Input file {args.input} is a symlink or contains in its path")
        update_payload_status(args.id, status="failed")
        sys.exit(0)
        
    #ensure the video file exists or handle URLs/webcam
    if not os.path.exists(args.input) and not args.input.startswith(("rtsp://", "/dev/video")):
        if args.input.isdigit():
            args.input = "/dev/video" + args.input 
            logging.info(f"Input is a device index or webcam: {args.input}. Skipping file download.")
        else:
            logging.error( "Input video file not found and no webcam detected. Please provide a valid input source.")
            update_payload_status(args.id, status="failed")
            exit(1)
    elif os.path.exists(args.input):
        if not is_valid_video_file(args.input):
            logging.error(f"Input file '{args.input}' is not a valid video file. Please provide a valid video file.")
            update_payload_status(args.id, status="failed")
            exit(1)
        filename = os.path.splitext(os.path.basename(args.input))[0]
        rtsp_url = f"{RSTP_SERVER_URL}/{filename}-{args.id}"
        logging.info(f"Hosting RTSP stream at: {rtsp_url}") 
        ffmpeg_command  = [
            "ffmpeg",
            "-re",
            "-stream_loop",
            "-1",
            "-i",
            args.input,
            "-c",
            "copy",
            "-f",
            "rtsp",
            "-rtsp_transport",
            "tcp",
            rtsp_url,
        ]
        args.input = rtsp_url
        
        try:
            ffmpeg_process = sp.Popen(
                ffmpeg_command, stdout=sp.DEVNULL, stderr=sp.DEVNULL
            )
            logging.info(f"Started RTSP streaming with PID: {ffmpeg_process.pid}")
        except sp.CalledProcessError as e:
            logging.error(f"Failed to host RTSP stream: {e}")
        
    if not is_rtsp_stream_running(args.input, retries=5, delay=1):
        logging.error("RTSP stream is not running after multiple attempts. Exiting...")
        update_payload_status(args.id, status="failed")
        exit(1)
    else:
        time.sleep(5)
             
    
    if (args.model in SEGMENTATION_MODELS): 
        model_status = export_model(
            model_name=args.model, model_parent_dir=args.model_parent_dir
        )
        if not model_status:
            update_payload_status(args.id, status="failed")
            exit(1)
     
        model_dir = Path(args.model_parent_dir) / args.model
            
        xml_files = list((model_dir/args.model_precision).glob("*.xml"))
        if xml_files:
            model_full_path=xml_files[0]
        else:
            logging.error(f"No model files found in {model_dir / args.model_precision}")
            update_payload_status(args.id, status="failed")
            exit(1)
            
        proc_files = list(model_dir.glob("*.json"))
        if proc_files:
            model_proc_path = proc_files[0]
            logging.info(f"Found model proc file: {model_proc_path}")
        else:
            logging.warning(f"No model proc file found in {model_dir}")
        
        label_files = list(model_dir.glob("*.txt"))
        if label_files:
            model_label_path = label_files[0]
            logging.info(f"Found model label file: {model_label_path}")
        else:
            logging.warning(f"No model label file found in {model_dir}")
        
    elif args.model.endswith(".zip"):
        model_zipfile_name = Path(args.model).stem 
        model_extract_dir = MODEL_DIR/model_zipfile_name
        if not model_extract_dir.exists():
            logging.info(f"Extracting {args.model} to {model_extract_dir}")
            try:
                with zipfile.ZipFile(args.model, "r") as zip_ref:
                    zip_ref.extractall(model_extract_dir)
            except Exception as e:
                logging.error(f"Failed to extract zip file {args.model}: {e}")
                update_payload_status(args.id, status="failed")
                exit(1)
        else:
            logging.info(
                f"Model directory {model_extract_dir} already exists, skipping extraction."
            )
        
        # Find for .xml file
        xml_files = list((model_extract_dir).glob("*.xml"))
        if not xml_files:
            logging.error(f"No model XML files found in {model_extract_dir}.")
            update_payload_status(args.id, status="failed")
            exit(1)
        model_full_path = xml_files[0]
        
        # Find model proc file
        proc_files = list(model_extract_dir.glob("*.json"))
        if proc_files:
            model_proc_path = proc_files[0]       
        else:
            logging.warning(f"No model processing file found in {model_extract_dir}")
       
        #find model label file
        label_files = list(model_extract_dir.glob("*.txt"))
        if label_files:
            model_label_path = proc_files[0]       
        else:
            logging.warning(f"No model processing file found in {model_extract_dir}")
    else:
        # Handle custom model uploaded to directory
        custom_model_path = CUSTOM_MODELS_DIR / args.model
        if not custom_model_path.exists():
            # Predefined model - construct path following shell script pattern
            model_status =  export_model(
                model_name=args.model, model_parent_dir=args.model_parent_dir
            )

            if not model_status:
                update_payload_status(args.id, status="failed")
                exit(1)
                
            model_full_path = (
                Path(args.model_parent_dir)
                / f"{args.model}-{args.model_precision}"
                / f"{args.model}.xml"
            )
            
        else:
            custom_model_files = list(custom_model_path.glob("*.xml"))
            if not custom_model_files:
                logging.error(f"No model XML files found in {custom_model_path}.")
                update_payload_status(args.id, status="failed")
                exit(1)
            model_full_path = custom_model_files[0]
            proc_files = list(custom_model_path.glob("*.json"))
            if proc_files:
                model_proc_path = proc_files[0]
                
            label_files = list(custom_model_path.glob("*.txt"))
            if label_files:
                model_label_path = proc_files[0]


    # Build the pipeline
    pipeline = build_pipeline(
        tcp_port=args.tcp_port,
        inference_mode=args.inference_mode,
        input=args.input,
        model_full_path=str(model_full_path),
        model_proc_path=str(model_proc_path) if model_proc_path else None,
        model_label_path=str(model_label_path) if model_label_path else None,
        device=args.device,
        decode_device=args.decode_device,
        number_of_streams=args.number_of_streams,
    )

    # Start the pipeline
    logging.info("Starting the pipeline...")
    try:
        update_payload_status(args.id, status="active")
        run_pipeline(pipeline)
    except KeyboardInterrupt:
        logging.info("Pipeline interrupted. Exiting...")
    except Exception as e:
        update_payload_status(args.id, status="failed")
        logging.error(f"An error occurred while running the pipeline: {e}")
        
@app.get("/result")
def get_mjpeg_stream():
    """
    Serve the MJPEG stream as an HTTP response
    """
    try:
        return StreamingResponse(
            mjpeg_stream(port=args.tcp_port),
            media_type="multipart/x-mixed-replace; boundary=frame",
        )
    except Exception as e:
        logging.error(f"Failed to serve MJPEG stream: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Failed to serve MJPEG stream: {str(e)}"}
        )

@app.get("/api/metrics")
def get_pipeline_metrics():
    """
    Return the current pipeline metrics
    """
    try:
        result = {
            "data": app.state.pipeline_metrics,
            "status": "success",
        }
        return JSONResponse(result)
    except Exception as e:
        logging.error(f"Error retrieving metrics: {e}")
        return JSONResponse(
            {"status": False, "message": "An error occurred while retrieving metrics"}
        )
            

if __name__ == "__main__":
    uvicorn.run('main:app', host="127.0.0.1", port=args.port)